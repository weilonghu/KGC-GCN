{
    "learning_rate": 1e-2,
    "weight_decay": 5e-4,
    "clip_grad": 1.0,

    "epoch_num": 10000,
    "min_epoch_num": 1000,
    "eval_every": 500,
    "patience": 0.02,
    "patience_num": 10,

    "regularization": 1e-2,
    "emb_dim": 100,
    "batch_size": 2000,
    "negative_rate": 1,
    "dropout": 0.2,

    "load_pretrain": false
}